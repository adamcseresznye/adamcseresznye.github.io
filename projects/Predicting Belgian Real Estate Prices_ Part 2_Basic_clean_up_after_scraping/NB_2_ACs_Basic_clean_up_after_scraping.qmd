---
title: 'Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping'
author: Adam Cseresznye
date: '2023-11-04'
categories:
  - Predicting Belgian Real Estate Prices
jupyter: python3
toc: true
format:
  html:
    code-fold: true
    code-tools: true
---

![Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash](https://cf.bstatic.com/xdata/images/hotel/max1024x768/408003083.jpg?k=c49b5c4a2346b3ab002b9d1b22dbfb596cee523b53abef2550d0c92d0faf2d8b&o=&hp=1){fig-align="center" width=50%}

In part 1, we provided a brief introduction to the project's purpose. Now, in part 2, we will dive deeper into the data processing steps required after scraping. We will discuss the handling of numerical data, categorical variables, and boolean values. Additionally, we'll assess the data quality by examining the error log generated by the `Immowebscraper` class. Let's get to it!

::: {.callout-note}
You can access the project's app through its [Streamlit website](https://belgian-house-price-predictor.streamlit.app/).

:::

# Import data

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
#| tags: []
import time
from pathlib import Path

import numpy as np
import pandas as pd
from data import pre_process, utils
from lets_plot import *
from lets_plot.mapping import as_discrete

LetsPlot.setup_html()
```

# Data Pre-cleaning steps

In the upcoming section, we will delve into fundamental post-web scraping procedures. While scraping, we acquired 50 features, making our dataset information-rich. However, there's significant work ahead due to the data's lack of cleanliness. This entails tasks such as dtype conversion, column parsing to extract numerical values, and the transformation of Boolean values into binary variables.  
Here is the dataset we have gathered through web scraping:

```{python}
for filename in utils.Configuration.RAW_DATA_PATH.glob("*.gzip"):
    if "for_NB2" in filename.stem:
        df = pd.read_parquet(filename)
print(df.shape)
df.head().style.set_sticky(axis=0)
```

Based on the dataset, we've identified two primary tasks that need to be performed across multiple columns:

1. **Handling Numerical Columns:** This involves extracting numerical data using regex and converting it to float format.

2. **Dealing with Binary Columns:** Many columns contain binary values, such as "Yes" and "No." We can easily convert these columns to boolean data types instead of string representations.

3. **Special Handling for Certain Columns:** Some columns, like "flood_zone_type" and "connection_to_sewer_network," also have low cardinality and should be converted to boolean values. However, their values do not align with the typical "True" and "False" boolean mapping. Instead, they require a unique dictionary mapping compared to the other boolean columns.

The `pre_process_dataframe` function below serves as a solid starting point. Some may suggest that breaking it into multiple subfunctions could improve maintainability and enable unit testing. However, for the time being, we'll maintain it in its current form.

```{python}
def pre_process_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Preprocesses a DataFrame by performing various data cleaning and transformation tasks.

    Args:
        df (pandas.DataFrame): The input DataFrame to be preprocessed.

    Returns:
        pandas.DataFrame: The preprocessed DataFrame.
    """

    def extract_numbers(df: pd.DataFrame, columns: list):
        """
        Extracts numeric values from specified columns in the DataFrame.

        Args:
            df (pandas.DataFrame): The DataFrame to extract values from.
            columns (list): List of column names to extract numeric values from.

        Returns:
            pandas.DataFrame: The DataFrame with extracted numeric values.
        """
        for column in columns:
            try:
                df[column] = df[column].str.extract(r"(\d+)").astype("float32")
            except Exception as e:
                print(f"Error processing column {column}: {e}")
        return df

    def map_values(df: pd.DataFrame, columns: list):
        """
        Maps boolean values in specified columns to True, False, or None.

        Args:
            df (pandas.DataFrame): The DataFrame to map values in.
            columns (list): List of column names with boolean values to be mapped.

        Returns:
            pandas.DataFrame: The DataFrame with mapped boolean values.
        """
        for column in columns:
            try:
                df[column] = df[column].map({"Yes": True, None: False, "No": False})
            except Exception as e:
                print(f"Error processing column {column}: {e}")
        return df

    number_columns = [
        "construction_year",
        "street_frontage_width",
        "number_of_frontages",
        "covered_parking_spaces",
        "outdoor_parking_spaces",
        "living_area",
        "living_room_surface",
        "kitchen_surface",
        "bedrooms",
        "bedroom_1_surface",
        "bedroom_2_surface",
        "bedroom_3_surface",
        "bathrooms",
        "toilets",
        "surface_of_the_plot",
        "width_of_the_lot_on_the_street",
        "garden_surface",
        "primary_energy_consumption",
        "co2_emission",
        "yearly_theoretical_total_energy_consumption",
    ]

    boolean_columns = [
        "basement",
        "furnished",
        "gas_water__electricity",
        "double_glazing",
        "planning_permission_obtained",
        "tv_cable",
        "dining_room",
        "proceedings_for_breach_of_planning_regulations",
        "subdivision_permit",
        "tenement_building",
        "possible_priority_purchase_right",
    ]

    return (
        df.sort_index(axis=1)
        .fillna(np.nan)
        .rename(
            columns=lambda column: column.lower()
            .replace(" ", "_")
            .replace("&", "")
            .replace(",", "")
        )
        .rename(columns={"coâ‚‚_emission": "co2_emission"})
        .pipe(lambda df: extract_numbers(df, number_columns))
        .pipe(lambda df: map_values(df, boolean_columns))
        .assign(
            flood_zone_type=lambda df: df.flood_zone_type.map(
                {
                    "Non flood zone": False,
                    "No": False,
                    "Possible flood zone": True,
                }
            ),
            connection_to_sewer_network=lambda df: df.connection_to_sewer_network.map(
                {
                    "Connected": True,
                    "Not connected": False,
                }
            ),
            as_built_plan=lambda df: df.as_built_plan.map(
                {
                    "Yes, conform": True,
                    "No": False,
                }
            ),
            cadastral_income=lambda df: df.cadastral_income.str.split(" ", expand=True)[
                3
            ].astype("float32"),
            price=lambda df: df.price.str.rsplit(" ", expand=True, n=2)[1].astype(
                float
            ),
        )
    )


df_pre_processed = pre_process_dataframe(df)
df_pre_processed.head().style.set_sticky(axis=0)
```

A crucial task on our agenda is the thorough parsing of address information. This is where the `separate_address` function comes into play. While this function provides a solid foundation for parsing location details, we've found that achieving high accuracy using regex alone can be challenging. For this reason, we've chosen to leverage the Google Maps API to extract location details effectively.

The function that's integrated into our final pipeline is `get_location_details_from_google`, which can be located in the `pre_process.py` module. This step is essential for extracting key details such as the city, ZIP code, house number, and stre reliably with high accuracyet. As we all know, location plays a vital role in real estate price estimation.

It's worth mentioning that we've removed the original address field to eliminate redundant data and ensure our dataset is more streamlined and efficions.

```{python}
def separate_address(df: pd.DataFrame) -> pd.DataFrame:
    """Separates the address into city, street name, house number, and zip code.

    Args:
        df (pd.DataFrame): The DataFrame containing the address column.

    Returns:
        pd.DataFrame: The DataFrame with the address separated into different columns.
    """
    # Define a regular expression pattern to extract street, house number, and zip code
    pattern = r"(?P<street_name>.*?)\s*(?P<house_number>\d+\w*)?\s*(?P<zip>\d{4})"

    try:
        return df.assign(
            city=lambda df: df.address.str.rsplit("-", expand=True, n=1)[1].str.title(),
            **(lambda dfx: dfx.rename(columns={"address": "original_address"}))(
                df["address"].str.extract(pattern)
            ),
            street=lambda df: df.street_name.str.replace(
                r"[^a-zA-Z\s]", "", regex=True
            ),
        ).drop(columns=["street_name", "address"])
    except Exception as e:
        print(f"Error separating address: {e}")
        return df


finer_pre_cleaned = separate_address(df_pre_processed)
finer_pre_cleaned.head().style.set_sticky(axis=0)
```

# Inspecting data quality
## Reading in and inspecting the log file

Analyzing the error log file, we've identified a total of 3,515 errors encountered during the web scraping process on the Immoweb website. Let's delve into these errors to pinpoint the most common issues and address them accordingly.

```{python}
error_log = pd.read_table(
    utils.Configuration.RAW_DATA_PATH.joinpath("make_dataset_error_for_NB2.log"),
    header=None,
).rename(columns={0: "error"})

error_log
```

### Most common errors from log file

It's clear that a significant majority of the errors, accounting for 1,848 cases, result from the absence of tables on the pages. These errors are primarily found on listing ads and index pages. To address this issue, we've introduced an `if` clause into our method `extract_ads_from_given_page`, which can be found in the `make_dataset.py` module. The clause, `if "immoweb.be" in item and "https://www.immoweb.be/en/search" not in item`, enables us to filter out undesired pages that don't contain relevant table information for our ads. This not only helps mitigate errors but also speeds up the dataset collection process by reducing the number of pages we scrape.

Another category of errors, totaling 1,460 cases, is related to the presence of duplicate labels during processing. We may need to investigate this issue further at a later stage to ensure data quality and accuracy.

A smaller proportion of errors is linked to the "Empty data" message, primarily related to ads. Finally, the remaining errors encompass errorrs related to data type conversion. We can consider either leaving these columns as is, since the error is not that frequent, or removing these features altogether.

```{python}
(
    error_log.error.str.split("-", expand=True)[4]
    .str.rsplit(" ", n=1, expand=True)[0]
    .value_counts()
)
```

We've identified five columns responsible for the last set of data type conversion problems (See below):

1. Construction year
2. Number of frontages
3. Outdoor parking spaces
4. Covered parking spaces
5. Bedrooms


```{python}
(
    error_log.error.str.split("-", expand=True)[4]
    .str.rsplit(" ", n=1, expand=True)[0]
    .value_counts()[3:]
    .to_frame()
    .reset_index()
    .rename(columns={0: "error_type"})
    .assign(
        error_type=lambda df: df.error_type.str.split(",", expand=True)[2].str.split(
            " ", n=5, expand=True
        )[5]
    )
    .error_type.value_counts()
)
```

### Unique URLs from the error logs

Upon conducting a more comprehensive analysis of the URLs extracted from error messages, a noteworthy observation comes to light: we've encountered only 433 unique URLs. This suggests that the 3,515 errors are stemming from a relatively restricted set of web addresses.

Now, here's a question that arises: the website implies the presence of 10,000 ads on the page. However, given our successful extraction of only 3,906 ads, along with the 433 URLs associated with errors, there is a substantial disparity evident.

If you have any insights or hypotheses regarding this difference, I'd be eager to hear your thoughts and discuss potential reasons for this variation.

```{python}
(
    error_log.error.str.split("-", expand=True)[4]
    .str.rsplit(" ", n=1, expand=True)[1]
    .unique()
    .shape
)
```

## Inspecting the data itself

After eliminating rows where all values were missing and filtering for rows with non-missing prices, we've successfully refined our dataset to include 3,660 ads.

In our subsequent analysis, we focus on the features with the lowest percentage of missing da, just like we did in Part 1ta. Notably, "day of retrieval" and "price" are complete, with all values present. However, it's important to recognize that roughly one-third of the data related to "dining_room" and "office" is missing, highlighting the need for improving data completeness in these specific attributet.s.

```{python}
(
    finer_pre_cleaned.dropna(axis=0, how="all")
    .query("price.notna()")
    .notna()
    .sum()
    .sort_values()
    .div(3660)
    .mul(100)
    .round(1)
)
```

Our `filter_out_missing_indexes` function proves to be quite valuable in the post-processing of our scraped data. This function is located at the end of our pre-process chain saving our data to the INTERIM_DATA_PATH folder after we've completed pre-processing and removed missing values.

```{python}
def filter_out_missing_indexes(
    df: pd.DataFrame,
    filepath: Path = utils.Configuration.INTERIM_DATA_PATH.joinpath(
        f"{str(pd.Timestamp.now())[:10]}_Processed_dataset.parquet.gzip"
    ),
) -> pd.DataFrame:
    """
    Filter out rows with missing values in a DataFrame and save the processed dataset.

    This function filters out rows with all missing values (NaN) and retains only rows
    with non-missing values in the 'price' column. The resulting DataFrame is then saved
    in Parquet format with gzip compression.

    Args:
        df (pd.DataFrame): The input DataFrame.
        filepath (Path, optional): The path to save the processed dataset in Parquet format.
            Defaults to a timestamp-based filepath in the interim data directory.

    Returns:
        pd.DataFrame: The filtered DataFrame with missing rows removed.

    Example:
        To filter out missing rows and save the processed dataset:
        >>> data = pd.read_csv("raw_data.csv")
        >>> filtered_data = filter_out_missing_indexes(data)
        >>> print(filtered_data.head())

    Notes:
        - Rows with missing values in any column other than 'price' are removed.
        - The processed dataset is saved with gzip compression to conserve disk space.
    """
    processed_df = df.dropna(axis=0, how="all").query("price.notna()")
    processed_df.to_parquet(filepath, compression="gzip", index=False)
    return processed_df
```

It appears that we've successfully completed the data transformation phase of our scraped dataset. With the implementation of functions like `filter_out_missing_indexes`, alongside `pre_process_dataframe` and `separate_address`, we've assembled the essential tools required for preparing our dataset for the machine learning pipeline.

In Part 3, we'll provide a fundamental overview and characterization of the cleaned scraped data. We'll assess feature cardinality, examine distributions, and explore correlations among variables. I look forward to delving into these insights with you in the next installment. See you there!

