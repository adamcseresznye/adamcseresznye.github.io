{
  "hash": "43c4c696fa3f4b8fda977638026d07cb",
  "result": {
    "markdown": "---\ntitle: 'Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model'\nauthor: Adam Cseresznye\ndate: '2023-11-06'\ncategories:\n  - Predicting Belgian Real Estate Prices\ntoc: true\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n![Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash](https://cf.bstatic.com/xdata/images/hotel/max1024x768/408003083.jpg?k=c49b5c4a2346b3ab002b9d1b22dbfb596cee523b53abef2550d0c92d0faf2d8b&o=&hp=1){fig-align=\"center\" width=50%}\n\nIn the preceding Part 3, our emphasis was on establishing a fundamental understanding of our data through characterizing the cleaned scraped dataset. We delved into feature cardinality, distributions, and potential correlations with our target variableâ€”property price. Moving on to Part 4, our agenda includes examining essential sample pre-processing steps before modeling. We will craft the necessary pipeline, assess multiple algorithms, and ultimately select a suitable baseline model. Let's get started!\n\n::: {.callout-note}\nYou can access the project's app through its [Streamlit website](https://belgian-house-price-predictor.streamlit.app/).\n\n:::\n\n# Import data\n\n::: {.cell editable='true' slideshow='{\"slide_type\":\"\"}' tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport catboost\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport xgboost\nfrom data import utils\nfrom IPython.display import clear_output, display\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nfrom sklearn import (\n    compose,\n    dummy,\n    ensemble,\n    impute,\n    linear_model,\n    metrics,\n    model_selection,\n    pipeline,\n    preprocessing,\n    svm,\n    tree,\n)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tqdm import tqdm\n\nLetsPlot.setup_html()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n            <div id=\"P5drQV\"></div>\n            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n                if(!window.letsPlotCallQueue) {\n                    window.letsPlotCallQueue = [];\n                }; \n                window.letsPlotCall = function(f) {\n                    window.letsPlotCallQueue.push(f);\n                };\n                (function() {\n                    var script = document.createElement(\"script\");\n                    script.type = \"text/javascript\";\n                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.1/js-package/distr/lets-plot.min.js\";\n                    script.onload = function() {\n                        window.letsPlotCall = function(f) {f();};\n                        window.letsPlotCallQueue.forEach(function(f) {f();});\n                        window.letsPlotCallQueue = [];\n                        \n                    };\n                    script.onerror = function(event) {\n                        window.letsPlotCall = function(f) {};    // noop\n                        window.letsPlotCallQueue = [];\n                        var div = document.createElement(\"div\");\n                        div.style.color = 'darkred';\n                        div.textContent = 'Error loading Lets-Plot JS';\n                        document.getElementById(\"P5drQV\").appendChild(div);\n                    };\n                    var e = document.getElementById(\"P5drQV\");\n                    e.appendChild(script);\n                })()\n            </script>\n            \n```\n:::\n:::\n\n\n# Prepare dataframe before modelling\n## Read in the processed file\n\nAfter importing our preprocessed dataframe, a crucial step in our data refinement process involves the culling of certain columns. Specifically, we intend to exclude columns with labels such as \"external_reference,\" \"ad_url,\" \"day_of_retrieval,\" \"website,\" \"reference_number_of_the_epc_report,\" and \"housenumber.\" Our rationale behind this action is to enhance the efficiency of our model by eliminating potentially non-contributory features.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nutils.seed_everything(utils.Configuration.seed)\n\ndf = (\n    pd.read_parquet(\n        utils.Configuration.INTERIM_DATA_PATH.joinpath(\n            \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n        )\n    )\n    .sample(frac=1, random_state=utils.Configuration.seed)\n    .reset_index(drop=True)\n    .assign(price=lambda df: np.log10(df.price))\n    .drop(\n        columns=[\n            \"external_reference\",\n            \"ad_url\",\n            \"day_of_retrieval\",\n            \"website\",\n            \"reference_number_of_the_epc_report\",\n            \"housenumber\",\n        ]\n    )\n)\n\nprint(f\"Shape of dataframe after read-in a pre-processing: {df.shape}\")\nX = df.drop(columns=utils.Configuration.target_col)\ny = df[utils.Configuration.target_col]\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of dataframe after read-in a pre-processing: (3660, 50)\nShape of X: (3660, 49)\nShape of y: (3660,)\n```\n:::\n:::\n\n\n## Train-test split\n\nThe subsequent phase in our data preparation involves the partitioning of our dataset into training and testing subsets. To accomplish this, we'll leverage the `model_selection.train_test_split` method. This step ensures that we have distinct sets for model training and evaluation, a fundamental practice in machine learning.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.2, random_state=utils.Configuration.seed\n)\n\nprint(f\"Shape of X-train: {X_train.shape}\")\nprint(f\"Shape of X-test: {X_test.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X-train: (2928, 49)\nShape of X-test: (732, 49)\n```\n:::\n:::\n\n\n# Implementing the data-processing pipeline\n\nIn order to compare various machine learning algorithms effectively, our initial approach will involve constructing a straightforward pipeline. This pipeline's primary objective is to segregate columns based on their data types, recognizing the need for distinct preprocessing steps for continuous (numerical) and categorical variables. To facilitate this process within our scikit-learn pipeline, we will begin by implementing a custom class named `FeatureSelector`.\n\nThe rationale behind this is to establish a structured approach to feature handling. The `FeatureSelector` class will provide us with a streamlined means to access and process columns based on their data typess.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for selecting specific columns from a DataFrame.\n\n    This class inherits from the BaseEstimator and TransformerMixin classes from sklearn.base.\n    It overrides the fit and transform methods from the parent classes.\n\n    Attributes:\n        feature_names_in_ (list): The names of the features to select.\n        n_features_in_ (int): The number of features to select.\n\n    Methods:\n        fit(X, y=None): Fit the transformer. Returns self.\n        transform(X, y=None): Apply the transformation. Returns a DataFrame with selected features.\n    \"\"\"\n\n    def __init__(self, feature_names_in_):\n        \"\"\"\n        Constructs all the necessary attributes for the FeatureSelector object.\n\n        Args:\n            feature_names_in_ (list): The names of the features to select.\n        \"\"\"\n        self.feature_names_in_ = feature_names_in_\n        self.n_features_in_ = len(feature_names_in_)\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the transformer. This method doesn't do anything as no fitting is necessary.\n\n        Args:\n            X (DataFrame): The input data.\n            y (array-like, optional): The target variable. Defaults to None.\n\n        Returns:\n            self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"\n        Apply the transformation. Selects the features from the input data.\n\n        Args:\n            X (DataFrame): The input data.\n            y (array-like, optional): The target variable. Defaults to None.\n\n        Returns:\n            DataFrame: A DataFrame with only the selected features.\n        \"\"\"\n        return X.loc[:, self.feature_names_in_].copy(deep=True)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Selecting columns by dtypes\n\nnumerical_columns = X_train.head().select_dtypes(\"number\").columns.to_list()\ncategorical_columns = X_train.head().select_dtypes(\"object\").columns.to_list()\n```\n:::\n\n\nAddressing missing values is a crucial preliminary step in our machine learning pipeline, as certain algorithms are sensitive to data gaps. To handle this, we'll employ imputation techniques tailored to the data types of the columns.\n\nFor numerical columns, we'll adopt the \"median\" strategy for imputation. This approach involves replacing missing values with the median of the available data in the respective numerical column. It's a robust choice for handling missing values in numerical data as it's less sensitive to outliers.\n\nConversely, for categorical columns, we'll opt for imputation using the most frequent values in each column. By filling in missing categorical data with the mode (most common value) for that column, we ensure that the imputed values align with the existing categorical distribution, preserving the integrity of the categorical features.\n\nThis systematic approach to imputation sets a solid foundation for subsequent machine learning algorithms, ensuring that our dataset is well-prepared for analysis and modeling.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Prepare pipelines for corresponding columns:\nnumerical_pipeline = pipeline.Pipeline(\n    steps=[\n        (\"num_selector\", FeatureSelector(numerical_columns)),\n        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n        (\"std_scaler\", preprocessing.MinMaxScaler()),\n    ]\n)\n\ncategorical_pipeline = pipeline.Pipeline(\n    steps=[\n        (\"cat_selector\", FeatureSelector(categorical_columns)),\n        (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n        (\n            \"onehot\",\n            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n        ),\n    ]\n)\n```\n:::\n\n\nOnce we are satisfied with the individual pipelines designed for numerical and categorical feature processing, the next step involves merging them into a unified pipeline using the `FeatureUnion` method provided by `scikit-learn`.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Put all the pipelines inside a FeatureUnion:\ndata_preprocessing_pipeline = pipeline.FeatureUnion(\n    n_jobs=-1,\n    transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ],\n)\n```\n:::\n\n\n# Compare the performance of several algorithms\n\nBringing all these components together in our machine learning pipeline is the culmination of our data preparation and model evaluation process. \n\n1. **Algorithm Selection**Wc Choose a set of machine learning algorithms tt ha u want to evaluaek.\n\n2. **Data Split*We u*: Utilize the `ShuffleSplit` method to generate randomized indices for s ouryour data into training and test sets. This ensures randomness in data selection and is crucial for unbiased evaluation.\n\n3. **Model Training and Evaluation**: For each selected algorwe followfollow these steps:\n   - Fit the model on the training data.\n   - Evaluate the model using negative mean squared error (`neg_meeda_squar`d, root mean squared log error (`mean_squared_log_error`) and coefficient of determination (`r2_score`)_error`) as the scoring metric.\n   - Record the training and test scores, as well as the standard deviation of scores tthe o assss model s\n   - Measure the time taken to fit each model, which provides insights into computational eformance.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nwith warnings.catch_warnings():\n    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n    MLA = [\n        linear_model.LinearRegression(),\n        linear_model.SGDRegressor(),\n        linear_model.PassiveAggressiveRegressor(),\n        linear_model.RANSACRegressor(),\n        linear_model.Lasso(),\n        svm.SVR(),\n        ensemble.GradientBoostingRegressor(),\n        tree.DecisionTreeRegressor(),\n        ensemble.RandomForestRegressor(),\n        ensemble.ExtraTreesRegressor(),\n        ensemble.AdaBoostRegressor(),\n        catboost.CatBoostRegressor(silent=True),\n        lgb.LGBMRegressor(verbose=-1),\n        xgboost.XGBRegressor(verbosity=0),\n        dummy.DummyClassifier(),\n    ]\n\n    # note: this is an alternative to train_test_split\n    cv_split = model_selection.ShuffleSplit(\n        n_splits=10, test_size=0.3, train_size=0.6, random_state=0\n    )  # run model 10x with 60/30 split intentionally leaving out 10%\n\n    # create table to compare MLA metrics\n    MLA_columns = [\n        \"MLA Name\",\n        \"MLA Parameters\",\n        \"MLA Train RMSE Mean\",\n        \"MLA Test RMSE Mean\",\n        \"MLA Train RMSLE Mean\",\n        \"MLA Test RMSLE Mean\",\n        \"MLA Train R2 Mean\",\n        \"MLA Test R2 Mean\",\n        \"MLA Time\",\n    ]\n    MLA_compare = pd.DataFrame(columns=MLA_columns)\n\n    RMSLE = {\n        \"RMSLE\": metrics.make_scorer(metrics.mean_squared_log_error, squared=False)\n    }\n\n    # index through MLA and save performance to table\n    row_index = 0\n    for alg in tqdm(MLA):\n        # set name and parameters\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, \"MLA Name\"] = MLA_name\n        MLA_compare.loc[row_index, \"MLA Parameters\"] = str(alg.get_params())\n\n        model_pipeline = pipeline.Pipeline(\n            steps=[\n                (\"data_preprocessing_pipeline\", data_preprocessing_pipeline),\n                (\"model\", alg),\n            ]\n        )\n\n        cv_results = model_selection.cross_validate(\n            model_pipeline,\n            X_train,\n            y_train,\n            cv=cv_split,\n            scoring={\n                \"RMSLE\": RMSLE[\"RMSLE\"],\n                \"r2\": \"r2\",\n                \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n            },\n            return_train_score=True,\n        )\n\n        MLA_compare.loc[row_index, \"MLA Time\"] = cv_results[\"fit_time\"].mean()\n        MLA_compare.loc[row_index, \"MLA Train RMSE Mean\"] = cv_results[\n            \"train_neg_mean_squared_error\"\n        ].mean()\n        MLA_compare.loc[row_index, \"MLA Test RMSE Mean\"] = cv_results[\n            \"test_neg_mean_squared_error\"\n        ].mean()\n\n        MLA_compare.loc[row_index, \"MLA Train RMSLE Mean\"] = cv_results[\n            \"train_RMSLE\"\n        ].mean()\n        MLA_compare.loc[row_index, \"MLA Test RMSLE Mean\"] = cv_results[\n            \"test_RMSLE\"\n        ].mean()\n\n        MLA_compare.loc[row_index, \"MLA Train R2 Mean\"] = cv_results[\"train_r2\"].mean()\n        MLA_compare.loc[row_index, \"MLA Test R2 Mean\"] = cv_results[\"test_r2\"].mean()\n\n        row_index += 1\n\n        clear_output(wait=True)\n        # display(MLA_compare.sort_values(by=[\"MLA Test RMSLE Mean\"], ascending=True))\n(\n    MLA_compare.sort_values(by=[\"MLA Test RMSLE Mean\"], ascending=True)\n    .drop(columns=\"MLA Parameters\")\n    .convert_dtypes()\n    .set_index(\"MLA Name\")\n    .style.set_table_styles(\n        [\n            {\n                \"selector\": \"th.col_heading\",\n                \"props\": \"text-align: center; font-size: 1.0em;\",\n            },\n            {\"selector\": \"td\", \"props\": \"text-align: center;\"},\n            {\n                \"selector\": \"td:hover\",\n                \"props\": \"font-style: italic; color: black; font-weight:bold; background-color : #ffffb3;\",\n            },\n        ],\n        overwrite=False,\n    )\n    .format(precision=3, thousands=\",\", decimal=\".\")\n    .background_gradient(cmap=\"coolwarm\", axis=0)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [15:25<00:00, 31.35s/it]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [15:25<00:00, 61.71s/it]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style type=\"text/css\">\n#T_dc103 th.col_heading {\n  text-align: center;\n  font-size: 1.0em;\n}\n#T_dc103 td {\n  text-align: center;\n}\n#T_dc103 td:hover {\n  font-style: italic;\n  color: black;\n  font-weight: bold;\n  background-color: #ffffb3;\n}\n#T_dc103_row0_col0, #T_dc103_row0_col4 {\n  background-color: #bb1b2c;\n  color: #f1f1f1;\n}\n#T_dc103_row0_col1, #T_dc103_row0_col5, #T_dc103_row1_col1, #T_dc103_row1_col5, #T_dc103_row2_col0, #T_dc103_row2_col4, #T_dc103_row2_col6, #T_dc103_row8_col0, #T_dc103_row8_col4, #T_dc103_row12_col2, #T_dc103_row12_col3 {\n  background-color: #b40426;\n  color: #f1f1f1;\n}\n#T_dc103_row0_col2 {\n  background-color: #6a8bef;\n  color: #f1f1f1;\n}\n#T_dc103_row0_col3, #T_dc103_row2_col2, #T_dc103_row8_col2, #T_dc103_row9_col6, #T_dc103_row10_col6, #T_dc103_row11_col6, #T_dc103_row12_col0, #T_dc103_row12_col4, #T_dc103_row12_col6, #T_dc103_row13_col1, #T_dc103_row13_col5 {\n  background-color: #3b4cc0;\n  color: #f1f1f1;\n}\n#T_dc103_row0_col6 {\n  background-color: #96b7ff;\n  color: #000000;\n}\n#T_dc103_row1_col0, #T_dc103_row1_col4, #T_dc103_row5_col0, #T_dc103_row5_col4 {\n  background-color: #ba162b;\n  color: #f1f1f1;\n}\n#T_dc103_row1_col2, #T_dc103_row5_col2 {\n  background-color: #6282ea;\n  color: #f1f1f1;\n}\n#T_dc103_row1_col3 {\n  background-color: #4055c8;\n  color: #f1f1f1;\n}\n#T_dc103_row1_col6 {\n  background-color: #3c4ec2;\n  color: #f1f1f1;\n}\n#T_dc103_row2_col1, #T_dc103_row2_col5, #T_dc103_row3_col1, #T_dc103_row3_col5, #T_dc103_row4_col1, #T_dc103_row4_col5, #T_dc103_row5_col1, #T_dc103_row5_col5 {\n  background-color: #b70d28;\n  color: #f1f1f1;\n}\n#T_dc103_row2_col3, #T_dc103_row3_col3 {\n  background-color: #4961d2;\n  color: #f1f1f1;\n}\n#T_dc103_row3_col0, #T_dc103_row3_col4 {\n  background-color: #cb3e38;\n  color: #f1f1f1;\n}\n#T_dc103_row3_col2 {\n  background-color: #90b2fe;\n  color: #000000;\n}\n#T_dc103_row3_col6 {\n  background-color: #536edd;\n  color: #f1f1f1;\n}\n#T_dc103_row4_col0, #T_dc103_row4_col4 {\n  background-color: #b50927;\n  color: #f1f1f1;\n}\n#T_dc103_row4_col2 {\n  background-color: #5572df;\n  color: #f1f1f1;\n}\n#T_dc103_row4_col3 {\n  background-color: #4a63d3;\n  color: #f1f1f1;\n}\n#T_dc103_row4_col6 {\n  background-color: #3d50c3;\n  color: #f1f1f1;\n}\n#T_dc103_row5_col3 {\n  background-color: #4b64d5;\n  color: #f1f1f1;\n}\n#T_dc103_row5_col6 {\n  background-color: #f18f71;\n  color: #f1f1f1;\n}\n#T_dc103_row6_col0, #T_dc103_row6_col4 {\n  background-color: #c73635;\n  color: #f1f1f1;\n}\n#T_dc103_row6_col1, #T_dc103_row6_col5, #T_dc103_row7_col1, #T_dc103_row7_col5 {\n  background-color: #be242e;\n  color: #f1f1f1;\n}\n#T_dc103_row6_col2 {\n  background-color: #88abfd;\n  color: #000000;\n}\n#T_dc103_row6_col3 {\n  background-color: #6b8df0;\n  color: #f1f1f1;\n}\n#T_dc103_row6_col6, #T_dc103_row8_col6 {\n  background-color: #3e51c5;\n  color: #f1f1f1;\n}\n#T_dc103_row7_col0, #T_dc103_row7_col4, #T_dc103_row11_col2 {\n  background-color: #e0654f;\n  color: #f1f1f1;\n}\n#T_dc103_row7_col2, #T_dc103_row9_col2 {\n  background-color: #b9d0f9;\n  color: #000000;\n}\n#T_dc103_row7_col3 {\n  background-color: #6c8ff1;\n  color: #f1f1f1;\n}\n#T_dc103_row7_col6 {\n  background-color: #4358cb;\n  color: #f1f1f1;\n}\n#T_dc103_row8_col1, #T_dc103_row8_col5 {\n  background-color: #ca3b37;\n  color: #f1f1f1;\n}\n#T_dc103_row8_col3 {\n  background-color: #97b8ff;\n  color: #000000;\n}\n#T_dc103_row9_col0, #T_dc103_row9_col4 {\n  background-color: #e16751;\n  color: #f1f1f1;\n}\n#T_dc103_row9_col1, #T_dc103_row9_col5 {\n  background-color: #e7745b;\n  color: #f1f1f1;\n}\n#T_dc103_row9_col3 {\n  background-color: #c4d5f3;\n  color: #000000;\n}\n#T_dc103_row10_col0, #T_dc103_row10_col4 {\n  background-color: #ead4c8;\n  color: #000000;\n}\n#T_dc103_row10_col1, #T_dc103_row10_col5 {\n  background-color: #f08a6c;\n  color: #f1f1f1;\n}\n#T_dc103_row10_col2 {\n  background-color: #f7af91;\n  color: #000000;\n}\n#T_dc103_row10_col3 {\n  background-color: #f6a586;\n  color: #000000;\n}\n#T_dc103_row11_col0, #T_dc103_row11_col4 {\n  background-color: #94b6ff;\n  color: #000000;\n}\n#T_dc103_row11_col1, #T_dc103_row11_col5 {\n  background-color: #f39778;\n  color: #000000;\n}\n#T_dc103_row11_col3 {\n  background-color: #ed8366;\n  color: #f1f1f1;\n}\n#T_dc103_row12_col1 {\n  background-color: #f6bea4;\n  color: #000000;\n}\n#T_dc103_row12_col5 {\n  background-color: #f6bfa6;\n  color: #000000;\n}\n#T_dc103_row13_col0, #T_dc103_row13_col4 {\n  background-color: #d1493f;\n  color: #f1f1f1;\n}\n#T_dc103_row13_col2 {\n  background-color: #9bbcff;\n  color: #000000;\n}\n#T_dc103_row13_col3, #T_dc103_row14_col2, #T_dc103_row14_col3 {\n  background-color: #000000;\n  color: #f1f1f1;\n}\n#T_dc103_row13_col6 {\n  background-color: #8db0fe;\n  color: #000000;\n}\n#T_dc103_row14_col0 {\n  background-color: #80a3fa;\n  color: #f1f1f1;\n}\n#T_dc103_row14_col1 {\n  background-color: #c5d6f2;\n  color: #000000;\n}\n#T_dc103_row14_col4 {\n  background-color: #7ea1fa;\n  color: #f1f1f1;\n}\n#T_dc103_row14_col5 {\n  background-color: #c3d5f4;\n  color: #000000;\n}\n#T_dc103_row14_col6 {\n  background-color: #f59f80;\n  color: #000000;\n}\n</style>\n<table id=\"T_dc103\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_dc103_level0_col0\" class=\"col_heading level0 col0\" >MLA Train RMSE Mean</th>\n      <th id=\"T_dc103_level0_col1\" class=\"col_heading level0 col1\" >MLA Test RMSE Mean</th>\n      <th id=\"T_dc103_level0_col2\" class=\"col_heading level0 col2\" >MLA Train RMSLE Mean</th>\n      <th id=\"T_dc103_level0_col3\" class=\"col_heading level0 col3\" >MLA Test RMSLE Mean</th>\n      <th id=\"T_dc103_level0_col4\" class=\"col_heading level0 col4\" >MLA Train R2 Mean</th>\n      <th id=\"T_dc103_level0_col5\" class=\"col_heading level0 col5\" >MLA Test R2 Mean</th>\n      <th id=\"T_dc103_level0_col6\" class=\"col_heading level0 col6\" >MLA Time</th>\n    </tr>\n    <tr>\n      <th class=\"index_name level0\" >MLA Name</th>\n      <th class=\"blank col0\" >&nbsp;</th>\n      <th class=\"blank col1\" >&nbsp;</th>\n      <th class=\"blank col2\" >&nbsp;</th>\n      <th class=\"blank col3\" >&nbsp;</th>\n      <th class=\"blank col4\" >&nbsp;</th>\n      <th class=\"blank col5\" >&nbsp;</th>\n      <th class=\"blank col6\" >&nbsp;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_dc103_level0_row0\" class=\"row_heading level0 row0\" >CatBoostRegressor</th>\n      <td id=\"T_dc103_row0_col0\" class=\"data row0 col0\" >-0.003</td>\n      <td id=\"T_dc103_row0_col1\" class=\"data row0 col1\" >-0.013</td>\n      <td id=\"T_dc103_row0_col2\" class=\"data row0 col2\" >0.009</td>\n      <td id=\"T_dc103_row0_col3\" class=\"data row0 col3\" >0.017</td>\n      <td id=\"T_dc103_row0_col4\" class=\"data row0 col4\" >0.970</td>\n      <td id=\"T_dc103_row0_col5\" class=\"data row0 col5\" >0.875</td>\n      <td id=\"T_dc103_row0_col6\" class=\"data row0 col6\" >6.957</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row1\" class=\"row_heading level0 row1\" >LGBMRegressor</th>\n      <td id=\"T_dc103_row1_col0\" class=\"data row1 col0\" >-0.002</td>\n      <td id=\"T_dc103_row1_col1\" class=\"data row1 col1\" >-0.015</td>\n      <td id=\"T_dc103_row1_col2\" class=\"data row1 col2\" >0.007</td>\n      <td id=\"T_dc103_row1_col3\" class=\"data row1 col3\" >0.018</td>\n      <td id=\"T_dc103_row1_col4\" class=\"data row1 col4\" >0.978</td>\n      <td id=\"T_dc103_row1_col5\" class=\"data row1 col5\" >0.862</td>\n      <td id=\"T_dc103_row1_col6\" class=\"data row1 col6\" >0.224</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row2\" class=\"row_heading level0 row2\" >ExtraTreesRegressor</th>\n      <td id=\"T_dc103_row2_col0\" class=\"data row2 col0\" >-0.000</td>\n      <td id=\"T_dc103_row2_col1\" class=\"data row2 col1\" >-0.016</td>\n      <td id=\"T_dc103_row2_col2\" class=\"data row2 col2\" >0.000</td>\n      <td id=\"T_dc103_row2_col3\" class=\"data row2 col3\" >0.019</td>\n      <td id=\"T_dc103_row2_col4\" class=\"data row2 col4\" >1.000</td>\n      <td id=\"T_dc103_row2_col5\" class=\"data row2 col5\" >0.847</td>\n      <td id=\"T_dc103_row2_col6\" class=\"data row2 col6\" >24.998</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row3\" class=\"row_heading level0 row3\" >GradientBoostingRegressor</th>\n      <td id=\"T_dc103_row3_col0\" class=\"data row3 col0\" >-0.010</td>\n      <td id=\"T_dc103_row3_col1\" class=\"data row3 col1\" >-0.016</td>\n      <td id=\"T_dc103_row3_col2\" class=\"data row3 col2\" >0.015</td>\n      <td id=\"T_dc103_row3_col3\" class=\"data row3 col3\" >0.019</td>\n      <td id=\"T_dc103_row3_col4\" class=\"data row3 col4\" >0.910</td>\n      <td id=\"T_dc103_row3_col5\" class=\"data row3 col5\" >0.846</td>\n      <td id=\"T_dc103_row3_col6\" class=\"data row3 col6\" >2.016</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row4\" class=\"row_heading level0 row4\" >XGBRegressor</th>\n      <td id=\"T_dc103_row4_col0\" class=\"data row4 col0\" >-0.001</td>\n      <td id=\"T_dc103_row4_col1\" class=\"data row4 col1\" >-0.016</td>\n      <td id=\"T_dc103_row4_col2\" class=\"data row4 col2\" >0.005</td>\n      <td id=\"T_dc103_row4_col3\" class=\"data row4 col3\" >0.019</td>\n      <td id=\"T_dc103_row4_col4\" class=\"data row4 col4\" >0.990</td>\n      <td id=\"T_dc103_row4_col5\" class=\"data row4 col5\" >0.843</td>\n      <td id=\"T_dc103_row4_col6\" class=\"data row4 col6\" >0.307</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row5\" class=\"row_heading level0 row5\" >RandomForestRegressor</th>\n      <td id=\"T_dc103_row5_col0\" class=\"data row5 col0\" >-0.002</td>\n      <td id=\"T_dc103_row5_col1\" class=\"data row5 col1\" >-0.017</td>\n      <td id=\"T_dc103_row5_col2\" class=\"data row5 col2\" >0.007</td>\n      <td id=\"T_dc103_row5_col3\" class=\"data row5 col3\" >0.019</td>\n      <td id=\"T_dc103_row5_col4\" class=\"data row5 col4\" >0.978</td>\n      <td id=\"T_dc103_row5_col5\" class=\"data row5 col5\" >0.840</td>\n      <td id=\"T_dc103_row5_col6\" class=\"data row5 col6\" >19.347</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row6\" class=\"row_heading level0 row6\" >SVR</th>\n      <td id=\"T_dc103_row6_col0\" class=\"data row6 col0\" >-0.008</td>\n      <td id=\"T_dc103_row6_col1\" class=\"data row6 col1\" >-0.024</td>\n      <td id=\"T_dc103_row6_col2\" class=\"data row6 col2\" >0.013</td>\n      <td id=\"T_dc103_row6_col3\" class=\"data row6 col3\" >0.023</td>\n      <td id=\"T_dc103_row6_col4\" class=\"data row6 col4\" >0.925</td>\n      <td id=\"T_dc103_row6_col5\" class=\"data row6 col5\" >0.772</td>\n      <td id=\"T_dc103_row6_col6\" class=\"data row6 col6\" >0.391</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row7\" class=\"row_heading level0 row7\" >AdaBoostRegressor</th>\n      <td id=\"T_dc103_row7_col0\" class=\"data row7 col0\" >-0.020</td>\n      <td id=\"T_dc103_row7_col1\" class=\"data row7 col1\" >-0.024</td>\n      <td id=\"T_dc103_row7_col2\" class=\"data row7 col2\" >0.021</td>\n      <td id=\"T_dc103_row7_col3\" class=\"data row7 col3\" >0.023</td>\n      <td id=\"T_dc103_row7_col4\" class=\"data row7 col4\" >0.814</td>\n      <td id=\"T_dc103_row7_col5\" class=\"data row7 col5\" >0.768</td>\n      <td id=\"T_dc103_row7_col6\" class=\"data row7 col6\" >0.770</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row8\" class=\"row_heading level0 row8\" >DecisionTreeRegressor</th>\n      <td id=\"T_dc103_row8_col0\" class=\"data row8 col0\" >-0.000</td>\n      <td id=\"T_dc103_row8_col1\" class=\"data row8 col1\" >-0.035</td>\n      <td id=\"T_dc103_row8_col2\" class=\"data row8 col2\" >0.000</td>\n      <td id=\"T_dc103_row8_col3\" class=\"data row8 col3\" >0.028</td>\n      <td id=\"T_dc103_row8_col4\" class=\"data row8 col4\" >1.000</td>\n      <td id=\"T_dc103_row8_col5\" class=\"data row8 col5\" >0.664</td>\n      <td id=\"T_dc103_row8_col6\" class=\"data row8 col6\" >0.350</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row9\" class=\"row_heading level0 row9\" >PassiveAggressiveRegressor</th>\n      <td id=\"T_dc103_row9_col0\" class=\"data row9 col0\" >-0.020</td>\n      <td id=\"T_dc103_row9_col1\" class=\"data row9 col1\" >-0.076</td>\n      <td id=\"T_dc103_row9_col2\" class=\"data row9 col2\" >0.021</td>\n      <td id=\"T_dc103_row9_col3\" class=\"data row9 col3\" >0.033</td>\n      <td id=\"T_dc103_row9_col4\" class=\"data row9 col4\" >0.809</td>\n      <td id=\"T_dc103_row9_col5\" class=\"data row9 col5\" >0.278</td>\n      <td id=\"T_dc103_row9_col6\" class=\"data row9 col6\" >0.058</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row10\" class=\"row_heading level0 row10\" >SGDRegressor</th>\n      <td id=\"T_dc103_row10_col0\" class=\"data row10 col0\" >-0.065</td>\n      <td id=\"T_dc103_row10_col1\" class=\"data row10 col1\" >-0.093</td>\n      <td id=\"T_dc103_row10_col2\" class=\"data row10 col2\" >0.039</td>\n      <td id=\"T_dc103_row10_col3\" class=\"data row10 col3\" >0.045</td>\n      <td id=\"T_dc103_row10_col4\" class=\"data row10 col4\" >0.390</td>\n      <td id=\"T_dc103_row10_col5\" class=\"data row10 col5\" >0.114</td>\n      <td id=\"T_dc103_row10_col6\" class=\"data row10 col6\" >0.057</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row11\" class=\"row_heading level0 row11\" >Lasso</th>\n      <td id=\"T_dc103_row11_col0\" class=\"data row11 col0\" >-0.106</td>\n      <td id=\"T_dc103_row11_col1\" class=\"data row11 col1\" >-0.105</td>\n      <td id=\"T_dc103_row11_col2\" class=\"data row11 col2\" >0.049</td>\n      <td id=\"T_dc103_row11_col3\" class=\"data row11 col3\" >0.049</td>\n      <td id=\"T_dc103_row11_col4\" class=\"data row11 col4\" >0.000</td>\n      <td id=\"T_dc103_row11_col5\" class=\"data row11 col5\" >-0.001</td>\n      <td id=\"T_dc103_row11_col6\" class=\"data row11 col6\" >0.057</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row12\" class=\"row_heading level0 row12\" >DummyClassifier</th>\n      <td id=\"T_dc103_row12_col0\" class=\"data row12 col0\" >-0.145</td>\n      <td id=\"T_dc103_row12_col1\" class=\"data row12 col1\" >-0.147</td>\n      <td id=\"T_dc103_row12_col2\" class=\"data row12 col2\" >0.056</td>\n      <td id=\"T_dc103_row12_col3\" class=\"data row12 col3\" >0.056</td>\n      <td id=\"T_dc103_row12_col4\" class=\"data row12 col4\" >-0.372</td>\n      <td id=\"T_dc103_row12_col5\" class=\"data row12 col5\" >-0.400</td>\n      <td id=\"T_dc103_row12_col6\" class=\"data row12 col6\" >0.063</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row13\" class=\"row_heading level0 row13\" >LinearRegression</th>\n      <td id=\"T_dc103_row13_col0\" class=\"data row13 col0\" >-0.012</td>\n      <td id=\"T_dc103_row13_col1\" class=\"data row13 col1\" >-0.392</td>\n      <td id=\"T_dc103_row13_col2\" class=\"data row13 col2\" >0.016</td>\n      <td id=\"T_dc103_row13_col3\" class=\"data row13 col3\" ><NA></td>\n      <td id=\"T_dc103_row13_col4\" class=\"data row13 col4\" >0.887</td>\n      <td id=\"T_dc103_row13_col5\" class=\"data row13 col5\" >-2.707</td>\n      <td id=\"T_dc103_row13_col6\" class=\"data row13 col6\" >6.307</td>\n    </tr>\n    <tr>\n      <th id=\"T_dc103_level0_row14\" class=\"row_heading level0 row14\" >RANSACRegressor</th>\n      <td id=\"T_dc103_row14_col0\" class=\"data row14 col0\" >-0.115</td>\n      <td id=\"T_dc103_row14_col1\" class=\"data row14 col1\" >-0.234</td>\n      <td id=\"T_dc103_row14_col2\" class=\"data row14 col2\" ><NA></td>\n      <td id=\"T_dc103_row14_col3\" class=\"data row14 col3\" ><NA></td>\n      <td id=\"T_dc103_row14_col4\" class=\"data row14 col4\" >-0.084</td>\n      <td id=\"T_dc103_row14_col5\" class=\"data row14 col5\" >-1.240</td>\n      <td id=\"T_dc103_row14_col6\" class=\"data row14 col6\" >18.373</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe table above clearly shows that the `CatBoostRegressor` has performed exceptionally well, achieving the best scores in RMSE, RMSLE, and R2 on the test set. It has outperformed the `LGBMRegressor`, `ExtraTreesRegressor`, `GradientBoostingRegressor`, and even the `XGBRegressor`.\n\nIn the next section, we will dive deeper into optimizing our model. This will involve refining model settings, enhancing features, and employing techniques to improve our overall predictive accuracy. Looking forward to seeing you there!\n\n",
    "supporting": [
      "NB_4_ACs_Building_a_baseline_model_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}