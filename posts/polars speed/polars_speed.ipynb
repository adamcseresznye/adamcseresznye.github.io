{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'The Ultimate Speed Test: Pandas vs Polars '\n",
        "author: Adam Cseresznye\n",
        "date: '2023-08-13'\n",
        "categories:\n",
        "  - Polars\n",
        "  - Pandas\n",
        "toc: true\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "---"
      ],
      "id": "0b39a888"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#| fig-cap: Photo by Hans-Jurgen Mager on Unsplash\n",
        "#| label: fig-fig0\n",
        "\n",
        "![](hans-jurgen-mager-qQWV91TTBrE-unsplash.jpg){fig-align=\"center\" width=50%}\n",
        "\n",
        "Once I shared my first article about [\"Getting Started with Polars\"](https://adamcseresznye.github.io/blog/posts/polars/Polars_revised.html) I started thinking about something exciting: comparing the speed of Polars and Pandas. This curiosity was sparked by all the buzz around the brand-new [Pandas 2.0 release](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html), promising lightning-fast performance. Pandas 2.0 was announced to come packed with cool features, including the addition of Apache Arrow (pyarrow) as its backing memory format. The big perk of Apache Arrow is that it makes operations speedier and more memory-friendly. Naturally, this got me wondering: how does Pandas 2.0 measure up against Polars? Let's dive in and find out!\n",
        "\n",
        "::: {.callout-warning}\n",
        "\n",
        "Keep in mind: occasionally, I refer to some performance difference figures. Even though I repeated these experiments 100 times each, because computations can have a bit of randomness, there might be slight variations in the exact numbers.\n",
        "\n",
        ":::\n",
        "\n",
        "# Setup\n"
      ],
      "id": "c698c92c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "\n",
        "import numpy as np\n",
        "import timeit\n",
        "import random\n",
        "\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "\n",
        "pio.templates.default = \"presentation\"\n",
        "\n",
        "print(pd.__version__)\n",
        "print(pl.__version__)"
      ],
      "id": "77b70454",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "Throughout this article, we'll be working with the latest versions of the packages: Pandas 2.0.3 and Polars 0.18.11.\n",
        "\n",
        ":::\n",
        "\n",
        "# Generate the synthetic data\n",
        "\n",
        "For the purpose of benchmarking, we're going to create our own dataset. To ensure simplicity and uniformity, we'll generate a dataframe containing columns like name, year of birth, and city of residence ‚Äì pretty straightforward stuff. We're aiming for a dataset of 10,000 rows. While we could certainly run this benchmark on a dataframe with more than a million rows, our intention is to provide a realistic example. The code we'll provide allows you to effortlessly explore various scenarios according to your requirements. Feel free to use it or modify it if you want to perform your own test.\n"
      ],
      "id": "d671b460"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_sample_dataframe(n_rows: int, name: str, seed: int = 42) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a sample Polars DataFrame with the specified number of rows.\n",
        "\n",
        "    This function generates random data for several columns and creates a Polars\n",
        "    DataFrame with the specified number of rows. The generated data includes information\n",
        "    about names, birth years, cities, zip codes, incomes, marital status, number of children,\n",
        "    and car brands. The resulting DataFrame is written to CSV and Parquet files named\n",
        "    'sample.csv' and '{name}.parquet', respectively.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_rows : int\n",
        "        The number of rows to generate in the sample DataFrame.\n",
        "    name : str\n",
        "        The name to be used when writing the Parquet file.\n",
        "    seed : int, optional\n",
        "        The seed to use when generating random data. Default is 42.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pl.DataFrame\n",
        "        The first 5 rows of the generated sample DataFrame.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    data = {\n",
        "        \"name\": np.random.choice([\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"], n_rows),\n",
        "        \"born\": np.random.randint(1950, 1990, size=n_rows),\n",
        "        \"city\": np.random.choice(\n",
        "            [\"Shanghai\", \"San Francisco\", \"London\", \"Munich\", \"Mumbai\"], n_rows\n",
        "        ),\n",
        "        \"zip_code\": np.random.randint(10000, 99999, size=n_rows),\n",
        "        \"income\": np.random.normal(50000, 10000, size=n_rows),\n",
        "        \"is_married\": np.random.choice([True, False], n_rows),\n",
        "        \"children\": np.random.randint(0, 5, size=n_rows),\n",
        "        \"car\": np.random.choice([\"Ford\", \"BMW\", \"Toyota\", \"Bentley\", \"Mini\"], n_rows),\n",
        "    }\n",
        "    temp_df = pl.DataFrame(data)\n",
        "    temp_df.write_csv(\"sample.csv\")\n",
        "    temp_df.write_parquet(f\"{name}.parquet\")\n",
        "    return temp_df.head()\n",
        "\n",
        "\n",
        "create_sample_dataframe(n_rows=10_000, name=\"sample\", seed=1)"
      ],
      "id": "f602a446",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define helper class\n",
        "\n",
        "In order to streamline our code, we'll create a convenient helper class featuring two essential methods: `get_time` and `get_figure`. The `get_time` method leverages the `timeit.repeat` function to accurately gauge the execution time of our code. In this context, we'll set the parameters `number = 1` and `repeat = 100`. This configuration runs the operations 100 times, allowing us to derive key statistics like the mean, median, and standard deviation. \n",
        "\n",
        "On the other hand, the `get_figure` method takes these gathered results and generates a visually appealing image using Plotly. \n",
        "\n",
        "To bring it all together and keep things simple, we've designed the `run_test` function. This function orchestrates the entire process, seamlessly merging the steps outlined above.\n"
      ],
      "id": "6b7af079"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Timeit:\n",
        "    \"\"\"\n",
        "    A class for measuring the execution time of multiple functions.\n",
        "    This class provides methods for measuring the execution time of\n",
        "    multiple functions using the `timeit` module. The functions to be\n",
        "    tested are passed to the constructor as a dictionary, where the\n",
        "    keys are the names of the functions and the values are the functions\n",
        "    themselves. The `number` and `repeat` parameters control how many times\n",
        "    each function is executed and how many times the timing is repeated,\n",
        "    respectively.\n",
        "\n",
        "    The `get_time` method measures the execution time of each function and\n",
        "    stores the results in a dictionary, where the keys are the names of\n",
        "    the functions and the values are lists of execution times. The `get_figure`\n",
        "    method generates a box plot of the execution times using the `plotly.express.box`\n",
        "    function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    func_dict : dict\n",
        "        A dictionary of functions to test, where the keys are the names of the\n",
        "        functions and the values are the functions themselves.\n",
        "    number : int, optional\n",
        "        The number of times to execute each function in each timing run (default is 1).\n",
        "    repeat : int, optional\n",
        "        The number of times to repeat each timing run (default is 20).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    results : dict or None\n",
        "        A dictionary of execution times for each function, where the keys are the names\n",
        "        of the functions and the values are lists of execution times. This attribute is\n",
        "        `None` until `get_time` is called.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    get_time()\n",
        "        Measure the execution time of each function and store the results in the `results` attribute.\n",
        "    get_figure()\n",
        "        Generate a box plot of the execution times using `plotly.express.box`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func_dict, number=1, repeat=100):\n",
        "        self.func_dict = func_dict\n",
        "        self.number = number\n",
        "        self.repeat = repeat\n",
        "        self.results = None\n",
        "\n",
        "    def get_time(self):\n",
        "        \"\"\"\n",
        "        Measure the execution time of each function and store the results.\n",
        "        This method uses the `timeit.repeat` function to measure the execution time of each function\n",
        "        in `func_dict`. The results are stored in a dictionary, where the keys are the names of\n",
        "        the functions and the values are lists of execution times. This dictionary is also stored\n",
        "        in the `results` attribute of the `Timeit` object.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary of execution times for each function.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for k, v in self.func_dict.items():\n",
        "            results[k] = timeit.repeat(v, number=self.number, repeat=self.repeat)\n",
        "        self.results = results\n",
        "        return results\n",
        "\n",
        "    def get_figure(self):\n",
        "        \"\"\"\n",
        "        Generate a box plot of the execution times.\n",
        "        This method uses `plotly.express.box` to generate a box plot of the execution times\n",
        "        stored in `results`.If `results` is `None`, this method raises a `ValueError`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        plotly.graph_objs.Figure\n",
        "            A box plot figure object.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If no results are available (i.e., if `get_time` has not been called).\n",
        "        \"\"\"\n",
        "        if self.results is None:\n",
        "            raise ValueError(\"No results available. Please run get_time first.\")\n",
        "        df = pd.DataFrame(self.results)\n",
        "        mean_times = df.median() * 1000\n",
        "        title = f'Median Execution Times: <br><sup>{\", \".join([f\"{k}={v:.6f}ms\" for k,v in mean_times.items()])}</sup>'\n",
        "        return px.box(\n",
        "            df,\n",
        "            points=\"all\",\n",
        "            labels={\"value\": \"time (sec)\", \"variable\": \"\"},\n",
        "            title=title,\n",
        "            width=600,\n",
        "            height=500,\n",
        "        )\n",
        "\n",
        "\n",
        "def run_test(test: dict, name: str):\n",
        "    \"\"\"\n",
        "    Run a timing test on a dictionary of functions using the Timeit class and\n",
        "    return the results as a DataFrame.\n",
        "\n",
        "    This function takes a dictionary of functions as input, where the keys are\n",
        "    the names of the functions and the values are the functions themselves. It creates\n",
        "    a Timeit object with this dictionary and uses it to measure the execution time of\n",
        "    each function. The results are then displayed as a box plot using the `get_figure`\n",
        "    method of the Timeit object and returned as a Pandas DataFrame with columns renamed\n",
        "    according to the `name` argument.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    test : dict\n",
        "        A dictionary of functions to test, where the keys are the names of the functions\n",
        "        and the values are the functions themselves.\n",
        "    name : str\n",
        "        A string used to rename the columns of the returned DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A DataFrame containing the results of the timing test, with columns renamed\n",
        "        according to the `name` argument.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a Timeit object\n",
        "    t = Timeit(test)\n",
        "    # Measure the execution time of the functions\n",
        "    results = t.get_time()\n",
        "    # Generate a box plot of the execution times\n",
        "    fig = t.get_figure()\n",
        "    fig.show()\n",
        "\n",
        "    return pd.DataFrame(results).rename(\n",
        "        columns={\"Pandas\": f\"{name}_pandas\", \"Polars\": f\"{name}_polars\"}\n",
        "    )"
      ],
      "id": "ab9c5128",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading in data From a CSV File and parquet\n",
        "\n",
        "## CSV\n",
        "\n",
        "Reading CSV files from disk is a task that data scientists often find themselves doing. Now, let's see how these two libraries compare for this particular job. To maximize the blazing-fast data handling capabilities of PyArrow, we'll equip Pandas with the `engine=\"pyarrow\"` and `dtype_backend=\"pyarrow\"` arguments. Let's see how these choices shape the performance!\n"
      ],
      "id": "52651f01"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Reading in data From a CSV File\n",
        "#| label: fig-fig1\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: pd.read_csv(\n",
        "        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n",
        "    ),\n",
        "    \"Polars\": lambda: pl.read_csv(\"sample.csv\"),\n",
        "}\n",
        "read_csv = run_test(test_dict, \"Read csv\")"
      ],
      "id": "fig-fig1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of comparison, we'll also demonstrate the timeit function invoked using Jupyter cell magic. You'll notice that the numbers generated this way are quite closely aligned with ours.\n"
      ],
      "id": "013de817"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%timeit\n",
        "pd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")"
      ],
      "id": "f85f3b69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%timeit\n",
        "pl.read_csv(\"sample.csv\")"
      ],
      "id": "3a1df255",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## parquet\n",
        "\n",
        "Now, let's read the data in Parquet format.\n"
      ],
      "id": "e7fa809c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "#| fig-cap: Reading in data From a parquet File\n",
        "#| label: fig-fig2\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: pd.read_parquet(\n",
        "        \"sample.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n",
        "    ),\n",
        "    \"Polars\": lambda: pl.read_parquet(\"sample.parquet\"),\n",
        "}\n",
        "read_parquet = run_test(test_dict, \"Read parquet\")"
      ],
      "id": "fig-fig2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "Polars unquestionably wins this round, it can boast a speed advantage of 2 to 4 times over Pandas.\n",
        ":::\n",
        "\n",
        "# Selecting Columns\n",
        "\n",
        "Alright, let's spice things up a bit and select some columns to see which library races ahead in terms of speed!\n",
        "\n",
        "## Files awaiting in-memory reading\n",
        "\n",
        "A clever approach to conserve memory and enhance speed involves reading only the columns essential for operations. Consider a scenario where we're interested in displaying just the names from this dataset. The big question now: how do these libraries measure up in terms of speed? Let's find out!\n"
      ],
      "id": "d185b172"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Selecting Columns from a File Not Yet in Memory\n",
        "#| label: fig-fig3\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: pd.read_csv(\n",
        "        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\", usecols=[\"name\"]\n",
        "    ),\n",
        "    \"Polars\": lambda: pl.scan_csv(\"sample.csv\").select(pl.col(\"name\")).collect(),\n",
        "}\n",
        "select_col_not_in_memory = run_test(test_dict, \"Select column (not in memory)\")"
      ],
      "id": "fig-fig3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File is in memory\n",
        "\n",
        "As anticipated, Polars continues to showcase its swiftness. It's worth highlighting the usage of the `lazy` and `collect` methods in Polars. These nifty tools grant us access to the library's clever query optimization techniques, which play a pivotal role in significantly enhancing performance. OK, one step further: suppose our files are already loaded into memory. Would there still be a distinction in performance under this circumstance?\n"
      ],
      "id": "2af87bc4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "df_pandas = pd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n",
        "df_polars = pl.read_csv(\"sample.csv\")"
      ],
      "id": "d98fd331",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "#| fig-cap: Selecting Columns from a File Already in Memory\n",
        "#| label: fig-fig4\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: df_pandas.loc[:, \"name\"],\n",
        "    \"Polars\": lambda: df_polars.lazy().select(pl.col(\"name\")).collect(),\n",
        "}\n",
        "select_col_in_memory = run_test(test_dict, \"Select column\")"
      ],
      "id": "fig-fig4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "While Polars showed a significant speed advantage for tasks involving pre-read files, both libraries perform similarly when the files are already in memory.\n",
        ":::\n",
        "\n",
        "# Filtering Rows\n",
        "\n",
        "Now, let's explore the scenario where we filter our dataset based on one or more column values \n",
        "\n",
        "## Based on one condition\n",
        "\n",
        "For our simple scenario, we'll be narrowing down our focus to filter data based on individuals with the name \"David\".\n"
      ],
      "id": "9d842afe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Filtering Rows Based on One Condition\n",
        "#| label: fig-fig5\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (df_pandas.query(\"name=='David'\")),\n",
        "    \"Polars\": lambda: (df_polars.lazy().filter((pl.col(\"name\") == \"David\")).collect()),\n",
        "}\n",
        "filter_row_one_condition = run_test(test_dict, \"Filter (simple)\")"
      ],
      "id": "fig-fig5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Based on multiple conditions\n",
        "\n",
        "Now, for a more intricate challenge, we're going to dive into querying the data to extract individuals who meet specific criteria: those named David, born after 1980, residing in a city other than London, married, and with three children.\n"
      ],
      "id": "2a31b7bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Filtering Rows Based on Multiple Condition\n",
        "#| label: fig-fig6\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (\n",
        "        df_pandas.query(\n",
        "            \"name=='David' and born>1980 and city != 'London' or is_married == True and children >= 3\"\n",
        "        )\n",
        "    ),\n",
        "    \"Polars\": lambda: (\n",
        "        df_polars.lazy()\n",
        "        .filter(\n",
        "            (pl.col(\"name\") == \"David\")\n",
        "            & (pl.col(\"born\") > 1980)\n",
        "            & (pl.col(\"city\") != \"London\")\n",
        "            | (pl.col(\"is_married\") == True) & (pl.col(\"children\") >= 3)\n",
        "        )\n",
        "        .collect()\n",
        "    ),\n",
        "}\n",
        "filter_row_multiple_condition = run_test(test_dict, \"Filter (complex)\")"
      ],
      "id": "fig-fig6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "Both libraries tackled this challenge quite well, yet Pandas struggled to keep pace with Polars. It's intriguing to observe that while Pandas required nearly twice the time for the more intricate task, Polars managed to complete it in almost the same amount of time. Parallelization in action.\n",
        "\n",
        ":::\n",
        "\n",
        "# Performing operations on columns\n",
        "\n",
        "Now, let's roll up our sleeves and dive into performing some operations on the columns.\n",
        "\n",
        "## Single operation\n",
        "\n",
        "As a single operation, we'll simply calculate the century in which these individuals were born.\n"
      ],
      "id": "04870202"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Performing a Singme Operation on a Column\n",
        "#| label: fig-fig7\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (df_pandas.assign(born=lambda df: df.born.div(100).round())),\n",
        "    \"Polars\": lambda: (\n",
        "        df_polars.lazy().with_columns((pl.col(\"born\") / 100).round()).collect()\n",
        "    ),\n",
        "}\n",
        "operate_one_column = run_test(test_dict, \"Operate (one column)\")"
      ],
      "id": "fig-fig7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple operations\n",
        "\n",
        "Let's also explore what happens when performing multiple operations on the columns. We'll mix things up with some string operations, mapping, and math calculations to see how these libraries handle it!\n"
      ],
      "id": "22139a9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Perfrming a Multiple Operation on Columns\n",
        "#| label: fig-fig8\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (\n",
        "        df_pandas.assign(\n",
        "            born=lambda df: df.born.div(100).round(),\n",
        "            name=lambda df: df.name.str.lower(),\n",
        "            city=lambda df: df.city.str.upper(),\n",
        "            zip_code=lambda df: df.zip_code.mul(2),\n",
        "            income=lambda df: df.income.div(10).astype(\"str\"),\n",
        "            is_married=lambda df: df.is_married.map({False: 0, True: 1}),\n",
        "            children=lambda df: df.children.astype(\"bool\"),\n",
        "            car=lambda df: df.car.str[0],\n",
        "        )\n",
        "    ),\n",
        "    \"Polars\": lambda: (\n",
        "        df_polars.lazy()\n",
        "        .with_columns(\n",
        "            [\n",
        "                (pl.col(\"born\") / 100).round(),\n",
        "                pl.col(\"name\").str.to_lowercase(),\n",
        "                pl.col(\"city\").str.to_uppercase(),\n",
        "                pl.col(\"zip_code\") * 2,\n",
        "                (pl.col(\"income\") / 10).cast(pl.Utf8),\n",
        "                pl.col(\"is_married\").map_dict({False: 0, True: 1}),\n",
        "                pl.col(\"children\").cast(pl.Boolean),\n",
        "                pl.col(\"car\").str.slice(0, length=1),\n",
        "            ]\n",
        "        )\n",
        "        .collect()\n",
        "    ),\n",
        "}\n",
        "operate_multiple_column = run_test(test_dict, \"Operate (more columns)\")"
      ],
      "id": "fig-fig8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "Once again, Polars takes the lead. While both libraries required more time for the task involving multiple operations, Polars demonstrated superior scalability in this scenario.\n",
        "\n",
        ":::\n",
        "\n",
        "# Concatenating Data\n",
        "\n",
        "Now, let's turn our attention to concatenating two datasets. Let the merging begin!\n"
      ],
      "id": "f6d411ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_pandas2 = df_pandas.copy(deep=True)\n",
        "df_polars2 = df_polars.clone()"
      ],
      "id": "93271e11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Concatenating Dataframes\n",
        "#| label: fig-fig9\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: pd.concat([df_pandas, df_pandas2], axis=0),\n",
        "    \"Polars\": lambda: pl.concat([df_polars, df_polars2], how=\"vertical\"),\n",
        "}\n",
        "concatenate = run_test(test_dict, \"Concatenate\")"
      ],
      "id": "fig-fig9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "Once more, Polars shines with a remarkable speed advantage.\n",
        "    \n",
        ":::\n",
        "\n",
        "# Aggregation\n",
        "\n",
        "## Simple \n",
        "\n",
        "Time to shift our attention to aggregation. First up, a simple task: let's calculate the mean income based on names. Then, for a bit more complexity, we'll dive into computing statistics involving the income, children, and car columns. Things are about to get interesting!\n"
      ],
      "id": "6ac58bbc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Performing a simple aggregation\n",
        "#| label: fig-fig10\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (df_pandas.groupby(\"name\").income.mean()),\n",
        "    \"Polars\": lambda: (df_polars.lazy().groupby(\"name\").mean().collect()),\n",
        "}\n",
        "aggregate_simple = run_test(test_dict, \"Aggregate (simple)\")"
      ],
      "id": "fig-fig10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More complex\n"
      ],
      "id": "5dc82a4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Performing a complex aggregation\n",
        "#| label: fig-fig11\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (\n",
        "        df_pandas.groupby([\"name\", \"car\", \"is_married\"]).agg(\n",
        "            born_min=(\"born\", min),\n",
        "            born_max=(\"born\", max),\n",
        "            income_mean=(\"income\", np.mean),\n",
        "            income_median=(\"income\", np.median),\n",
        "            children_mean=(\"children\", np.mean),\n",
        "            car_count=(\"car\", \"count\"),\n",
        "        )\n",
        "    ),\n",
        "    \"Polars\": lambda: (\n",
        "        df_polars.lazy()\n",
        "        .groupby([\"name\", \"car\", \"is_married\"])\n",
        "        .agg(\n",
        "            [\n",
        "                pl.col(\"born\").min().alias(\"born_min\"),\n",
        "                pl.col(\"born\").max().alias(\"born_max\"),\n",
        "                pl.col(\"income\").mean().alias(\"income_mean\"),\n",
        "                pl.col(\"income\").median().alias(\"income_median\"),\n",
        "                pl.col(\"children\").mean().alias(\"children_mean\"),\n",
        "                pl.col(\"car\").count().alias(\"car_count\"),\n",
        "            ]\n",
        "        )\n",
        "        .collect()\n",
        "    ),\n",
        "}\n",
        "aggregate_complex = run_test(test_dict, \"Aggregate (complex)\")"
      ],
      "id": "fig-fig11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "\n",
        "While Pandas showcased noteworthy speed for the simple aggregation, the more intricate task exposed significant disparities between the two libraries. Polars took a commanding lead in this scenario, presenting a considerably faster performance compared to Pandas.\n",
        ":::\n",
        "\n",
        "# Whole workflow\n",
        "\n",
        "We're going to tackle this in two versions. First, we'll use `pl.read_csv` for Polars, assessing the time it takes to complete the entire workflow on a DataFrame already in memory. Then, for the second version, we'll employ `pl.scan_csv` for Polars. This nifty function lets us lazily read data from a CSV file or multiple files using glob patterns.\n",
        "\n",
        "The cool part? With `pl.scan_csv`, the query optimizer can push down predicates and projections to the scan level. This nifty move has the potential to cut down on memory overhead. Let's dive into both versions and see how they stack up!\n",
        "\n",
        "## Using `pl.read_csv`\n"
      ],
      "id": "52629aac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Performing a representative workflow\n",
        "#| label: fig-fig12\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (\n",
        "        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n",
        "        .query(\"name=='David' and born>1980 and city != 'London' and children >= 3\")\n",
        "        .assign(\n",
        "            born=lambda df: df.born.div(100).round(),\n",
        "            name=lambda df: df.name.str.lower(),\n",
        "            city=lambda df: df.city.str.upper(),\n",
        "            zip_code=lambda df: df.zip_code.mul(2),\n",
        "            income=lambda df: df.income.div(10),\n",
        "            children=lambda df: df.children.astype(\"bool\"),\n",
        "            car=lambda df: df.car.str[0],\n",
        "        )\n",
        "        .groupby(\n",
        "            [\n",
        "                \"name\",\n",
        "                \"car\",\n",
        "            ]\n",
        "        )\n",
        "        .agg(\n",
        "            born_min=(\"born\", min),\n",
        "            born_max=(\"born\", max),\n",
        "            income_mean=(\"income\", np.mean),\n",
        "            income_median=(\"income\", np.median),\n",
        "            children_mean=(\"children\", np.mean),\n",
        "            car_count=(\"car\", \"count\"),\n",
        "        )\n",
        "    ),\n",
        "    \"Polars\": lambda: (\n",
        "        df_polars.lazy()\n",
        "        .select(cs.all() - cs.ends_with(\"married\"))\n",
        "        .filter(\n",
        "            (pl.col(\"name\") == \"David\")\n",
        "            & (pl.col(\"born\") > 1980)\n",
        "            & (pl.col(\"city\") != \"London\")\n",
        "            & (pl.col(\"children\") >= 3)\n",
        "        )\n",
        "        .with_columns(\n",
        "            [\n",
        "                (pl.col(\"born\") / 100).round(),\n",
        "                pl.col(\"name\").str.to_lowercase(),\n",
        "                pl.col(\"city\").str.to_uppercase(),\n",
        "                pl.col(\"zip_code\") * 2,\n",
        "                (pl.col(\"income\") / 10),\n",
        "                pl.col(\"children\").cast(pl.Boolean),\n",
        "                pl.col(\"car\").str.slice(0, length=1),\n",
        "            ]\n",
        "        )\n",
        "        .groupby(\n",
        "            [\n",
        "                \"name\",\n",
        "                \"car\",\n",
        "            ]\n",
        "        )\n",
        "        .agg(\n",
        "            [\n",
        "                pl.col(\"born\").min().alias(\"born_min\"),\n",
        "                pl.col(\"born\").max().alias(\"born_max\"),\n",
        "                pl.col(\"income\").mean().alias(\"income_mean\"),\n",
        "                pl.col(\"income\").median().alias(\"income_median\"),\n",
        "                pl.col(\"children\").mean().alias(\"children_mean\"),\n",
        "                pl.col(\"car\").count().alias(\"car_count\"),\n",
        "            ]\n",
        "        )\n",
        "        .collect()\n",
        "    ),\n",
        "}\n",
        "whole_workflow_read_csv = run_test(test_dict, \"Whole workflow\")"
      ],
      "id": "fig-fig12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `pl.scan_csv`\n"
      ],
      "id": "61751f8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Performing a representative workflow (using `pl_scan_csv` for Polars)\n",
        "#| label: fig-fig13\n",
        "\n",
        "test_dict = {\n",
        "    \"Pandas\": lambda: (\n",
        "        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n",
        "        .query(\"name=='David' and born>1980 and city != 'London' and children >= 3\")\n",
        "        .assign(\n",
        "            born=lambda df: df.born.div(100).round(),\n",
        "            name=lambda df: df.name.str.lower(),\n",
        "            city=lambda df: df.city.str.upper(),\n",
        "            zip_code=lambda df: df.zip_code.mul(2),\n",
        "            income=lambda df: df.income.div(10),\n",
        "            children=lambda df: df.children.astype(\"bool\"),\n",
        "            car=lambda df: df.car.str[0],\n",
        "        )\n",
        "        .groupby(\n",
        "            [\n",
        "                \"name\",\n",
        "                \"car\",\n",
        "            ]\n",
        "        )\n",
        "        .agg(\n",
        "            born_min=(\"born\", min),\n",
        "            born_max=(\"born\", max),\n",
        "            income_mean=(\"income\", np.mean),\n",
        "            income_median=(\"income\", np.median),\n",
        "            children_mean=(\"children\", np.mean),\n",
        "            car_count=(\"car\", \"count\"),\n",
        "        )\n",
        "    ),\n",
        "    \"Polars\": lambda: (\n",
        "        pl.scan_csv(\"sample.csv\")\n",
        "        .select(cs.all() - cs.ends_with(\"married\"))\n",
        "        .filter(\n",
        "            (pl.col(\"name\") == \"David\")\n",
        "            & (pl.col(\"born\") > 1980)\n",
        "            & (pl.col(\"city\") != \"London\")\n",
        "            & (pl.col(\"children\") >= 3)\n",
        "        )\n",
        "        .with_columns(\n",
        "            [\n",
        "                (pl.col(\"born\") / 100).round(),\n",
        "                pl.col(\"name\").str.to_lowercase(),\n",
        "                pl.col(\"city\").str.to_uppercase(),\n",
        "                pl.col(\"zip_code\") * 2,\n",
        "                (pl.col(\"income\") / 10),\n",
        "                pl.col(\"children\").cast(pl.Boolean),\n",
        "                pl.col(\"car\").str.slice(0, length=1),\n",
        "            ]\n",
        "        )\n",
        "        .groupby(\n",
        "            [\n",
        "                \"name\",\n",
        "                \"car\",\n",
        "            ]\n",
        "        )\n",
        "        .agg(\n",
        "            [\n",
        "                pl.col(\"born\").min().alias(\"born_min\"),\n",
        "                pl.col(\"born\").max().alias(\"born_max\"),\n",
        "                pl.col(\"income\").mean().alias(\"income_mean\"),\n",
        "                pl.col(\"income\").median().alias(\"income_median\"),\n",
        "                pl.col(\"children\").mean().alias(\"children_mean\"),\n",
        "                pl.col(\"car\").count().alias(\"car_count\"),\n",
        "            ]\n",
        "        )\n",
        "        .collect()\n",
        "    ),\n",
        "}\n",
        "whole_workflow_scan_csv = run_test(test_dict, \"Whole workflow (scan_csv)\")"
      ],
      "id": "fig-fig13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As evident, the utilization of `scan_csv` increased the required time by about 3-4 times. However, even with this increase, Polars still manages to maintain a substantial advantage of around 5 times faster than the entire workflow executed using Pandas.\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "When we consider the entirety of the data processing pipeline, irrespective of the file reading approach, Polars emerges as the victor. It consistently exhibits a considerable speed advantage compared to Pandas.\n",
        "\n",
        ":::\n",
        "\n",
        "# Putting it all together\n",
        "\n",
        "Time to bring together all the things we've explored! Let's sum up what we've learned.\n"
      ],
      "id": "77c49924"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Analyzing Speed Differences Between Pandas and Polars in Common Data Science Workflow Operations\n",
        "#| label: fig-fig14\n",
        "#| column: page\n",
        "\n",
        "\n",
        "summary = pd.concat(\n",
        "    [\n",
        "        read_csv,\n",
        "        read_parquet,\n",
        "        # select_col_not_in_memory,\n",
        "        select_col_in_memory,\n",
        "        filter_row_one_condition,\n",
        "        filter_row_multiple_condition,\n",
        "        operate_one_column,\n",
        "        operate_multiple_column,\n",
        "        concatenate,\n",
        "        aggregate_simple,\n",
        "        aggregate_complex,\n",
        "        whole_workflow_read_csv,\n",
        "        # whole_workflow_scan_csv\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "fig = (\n",
        "    summary.melt(var_name=\"experiment\", value_name=\"time_sec\")\n",
        "    .assign(\n",
        "        package=lambda df: df.experiment.str[-6:],\n",
        "        experiment=lambda df: df.experiment.str[:-7],\n",
        "    )\n",
        "    .groupby([\"experiment\", \"package\"])\n",
        "    .time_sec.mean()\n",
        "    .mul(1000)\n",
        "    .reset_index()\n",
        "    .pivot(columns=\"package\", values=\"time_sec\", index=\"experiment\")\n",
        "    .sort_values(\n",
        "        by=\"polars\",\n",
        "    )\n",
        "    .reset_index()\n",
        "    .pipe(\n",
        "        lambda df: px.bar(\n",
        "            df,\n",
        "            y=\"experiment\",\n",
        "            x=[\"pandas\", \"polars\"],\n",
        "            barmode=\"group\",\n",
        "            text_auto='.2s',\n",
        "            orientation=\"h\",\n",
        "            labels={\"value\": \"Average execution Time (msec)\", \"experiment\": \"\"},\n",
        "            title= f'<b>Speed Showdown in Data Workflow Operations</b> <br><sup><i>Pandas {pd.__version__} vs. Polars {pl.__version__}</i></sup>',\n",
        "            width=1000,\n",
        "            height=700,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "fig.update_xaxes(tickangle=90)\n",
        "fig.update_layout(yaxis=dict(automargin=True), legend_title_text=\"Library\")\n",
        "fig.update_traces(textposition='outside')"
      ],
      "id": "fig-fig14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Throughout our exploration, Polars has consistently outperformed Pandas. We intentionally focused on tasks that frequently arise in data analysis, encompassing file reading, column selection, filtering, and more. However, where Polars truly shines is in aggregation, capitalizing on its multiprocessing prowess. Take a glance at @fig-fig14, and you'll notice that reading files is the most time-consuming step for Polars. To address this (and reduce memory usage at the same time), we have the nifty `scan_csv` function that enables Polars to operate in a lazy mode, optimizing the entire data pipeline. It's clear that Polars packs a punch in the realm of data processing!\n",
        "\n",
        "Should you consider using Polars? Based on the findings of this experiment, it's evident that Polars can bring substantial benefits to every stage of your data processing. Its remarkable speed can make a significant impact. If you're open to investing time in understanding its API (which isn't drastically different from Pandas), I'm confident your projects will gain a significant speed boost. And don't forget, the [trajectory](https://www.crunchbase.com/funding_round/polars-seed--963c89d2) of the project seems promising, implying a bright future ahead.\n",
        "\n",
        "Whatever path you choose, ensure you become well-acquainted with your selected library and stay current with emerging technologies. And above all, relish the journey. Happy coding, and until next time! üêºüêçü§ìüíª\n"
      ],
      "id": "3b69baf7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}